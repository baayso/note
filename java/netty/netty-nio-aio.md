# Netty-NIO-AIO

## 1. 以下内容是从此网页评论中提取出来的：
* https://my.oschina.net/u/2385344/blog/1603648?p=6

## 2. 评论内容：
```
netty拒绝AIO主要是因为netty整体架构是reactor模型, 
而AIO是proactor模型, 混合在一起会非常混乱, 
要么把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来, 
再加上netty基本不看重windows, AIO在linux也是用reactor模型的epoll实现的, 
而且被JDK封装了一层不容易深度优化, 所以才放弃的.
另外AIO还有个缺点是接收数据需要预先分配缓存, 
而不是NIO那种需要接收时才需要分配缓存, 
所以对连接数量非常大但流量小的情况, 内存浪费很多.
```

```
"另外AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 
所以对连接数量非常大但流量小的情况, 内存浪费很多."
这是AIO相较于NIO一个比较大的劣势了
```

```
说起来, libuv是统一了reactor和proactor(epoll和iocp)接口, 
看了下iocp的实现, 发现可以传空缓冲区去异步接收, 回调时再用缓冲区去实际接收, 
感觉是把iocp当reactor去用了, 然而AIO并不支持用空缓冲区异步接收.
```

```
理论上可以只提供一个缓冲区来异步接收多个连接之一的数据, 
在AIO上就应该给AsynchronousChannelGroup加一个read(...,CompletionHandler)接口, 
而不是在AsynchronousSocketChannel上.
但实际上我还没看到有这样设计的底层网络接口.
```

```
nio的可以这样做的，tio之前，我写过一个talent-nio，
那时候是一个selector对应一个大的bytebuffer，收到数据后，把数据copy给相对应的通道，
然后清空这个大的bytebuffer继续新的数据接收
```

```
nio可以这么做, 但不是我说的proactor模型, 仍然需要先等信号再接收两步操作, 
而且仍然有select轮询或阻塞等待, 不是真正的异步.
我觉得最理想的情况是纯异步proactor模型, 
一次操作和一个buffer就可以异步接收多个连接的数据(当然一次操作只接收某一连接的), 这种接口之上的开发也简单.
```

```
受教了🙏 但对于这个预先分配缓存的问题，我觉得在NIO中也是需要的，
因为并不能确保每一次执行read都能解析出完整的消息，还是需要为每一个连接都分配缓存空间吧

关于消息不完整的缓存那是另一个问题了. 通常来说只要不是网络卡的严重或者客户端恶意, 
一条完整消息应该很快就能组成, 只要组成好了,构造出消息对象, 那么数据缓存就可以丢弃(或者回收到池里).
或者只用一个全局的大缓存来接收(并发接收的话每个线程一个), 消息解码不完整的应该由解码模块自己分配空间缓存.
```

```
仅仅停留在aio/nio语言模型之上，而忽略系统底层的io模型，很容易进入一个误区，
netty的作者正是看透了这层本质才放弃aio的，具体的原因 dwing0 已经说的够清楚了。
不过aio提供了一个好处就是可以跨平台，为不同平台的io模型提供了一个比较通用统一的处理模式，
屏蔽底层细节，而不用针对底层的io模型做深度定制优化
```
